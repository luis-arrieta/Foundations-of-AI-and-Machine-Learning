{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQ0sNgYDsmYurC6F1x/lTl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luis-arrieta/Foundations-of-AI-and-Machine-Learning/blob/main/Identify_errors_in_data_collection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "\n",
        "Detecting these errors in data collection is crucial to ensure the quality and reliability of your data, which directly impacts the effectiveness of any AI or ML model built on that data.\n",
        "\n",
        "This reading provides a detailed walkthrough of identifying common data collection errors. By following this guide, you will gain a better understanding of the techniques and tools used to detect and correct these errors.\n",
        "\n",
        "By the end of this walkthrough, you will be able to:\n",
        "\n",
        "- Describe the structure and characteristics of a dataset.\n",
        "\n",
        "- Identify and handle missing values effectively.\n",
        "\n",
        "- Detect and manage outliers using statistical methods.\n",
        "\n",
        "- Identify and correct data entry errors for consistency.\n",
        "\n",
        "- Validate data consistency and ensure high-quality datasets."
      ],
      "metadata": {
        "id": "Ds4v8NONWrB0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step-by-step guide to identifying errors in data collection**\n",
        "\n",
        "**Step 1: Understand the dataset**\n",
        "\n",
        "Before identifying errors, it’s important to first understand the dataset you’re working with. This involves examining the data structure, types of data fields, and any known characteristics of the data."
      ],
      "metadata": {
        "id": "4hUG3X0gW6iD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LgqpcnEWlgh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas DataFrame\n",
        "df = pd.read_csv('your_dataset.csv')  # Replace with your actual file path\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(df.head())\n",
        "\n",
        "# Check the data types of each column\n",
        "print(df.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step helps you to become familiar with the dataset by viewing a sample of the data and understanding the data types of each column. This initial inspection sets the stage for identifying potential issues in the data."
      ],
      "metadata": {
        "id": "EG117OesXCBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Identify missing values**\n",
        "\n",
        "Missing values are a common issue in data collection that can skew results if not handled properly.\n",
        "\n",
        "**Check for missing values**"
      ],
      "metadata": {
        "id": "CcOLdMyvXC8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in the dataset\n",
        "missing_values = df.isnull().sum()\n",
        "\n",
        "# Display columns with missing values\n",
        "print(missing_values[missing_values > 0])"
      ],
      "metadata": {
        "id": "1gWC7IO9XIU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The isnull().sum() function provides a quick count of missing values in each column. Columns with a significant number of missing values may require further investigation to determine the cause of these gaps.\n",
        "\n",
        "**Handle missing values**\n",
        "\n",
        "Once identified, missing values can be handled through several methods:\n",
        "\n",
        "**A. Remove rows/columns:**"
      ],
      "metadata": {
        "id": "SHlmFdrHXLMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned = df.dropna()  # Drops rows with any missing values"
      ],
      "metadata": {
        "id": "G1Nc3CHtXPyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**B. Fill missing values:**"
      ],
      "metadata": {
        "id": "wWBYNb2nXTC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filled = df.fillna(df.mean())  # Fills missing numeric values with the mean of the column"
      ],
      "metadata": {
        "id": "Nu_ysXF5XV63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending on the context and extent of the missing data, you may choose to remove rows or columns with missing values or fill them with an appropriate statistic, such as the mean, median, or mode\n",
        "\n",
        "**Step 3: Detect outliers**\n",
        "\n",
        "Outliers can indicate errors in data collection, particularly if they deviate significantly from expected values.\n",
        "\n",
        "**Use descriptive statistics and visualization to detect outliers**\n",
        "\n",
        "Descriptive statistics such as the mean and standard deviation, along with visual tools such as box plots, help to identify outliers. Z-scores quantify how far a data point is from the mean, with a Z-score greater than 3 typically considered an outlier."
      ],
      "metadata": {
        "id": "WgDburvUXYPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "# Use descriptive statistics to identify potential outliers\n",
        "print(df.describe())\n",
        "\n",
        "# Visualize data to spot outliers using box plots\n",
        "df.boxplot(column=['Column1', 'Column2'])  # Replace with actual column names\n",
        "plt.show()\n",
        "\n",
        "# Calculate Z-scores to identify outliers\n",
        "z_scores = np.abs(stats.zscore(df.select_dtypes(include=[np.number])))\n",
        "\n",
        "# Find rows with Z-scores greater than 3\n",
        "outliers = (z_scores > 3).all(axis=1)\n",
        "print(df[outliers])"
      ],
      "metadata": {
        "id": "mfAK6tHtXfBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handle outliers**\n",
        "\n",
        "Outliers can be handled by either removing or transforming them:\n",
        "\n",
        "**A. Remove outliers:**"
      ],
      "metadata": {
        "id": "PEqDn4TIXg1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_no_outliers = df[(z_scores < 3).all(axis=1)]"
      ],
      "metadata": {
        "id": "caKzookzXj2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**B. Transform outliers:**"
      ],
      "metadata": {
        "id": "R-DNyD1AXlSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Column1'] = np.where(df['Column1'] > upper_limit, upper_limit, df['Column1'])"
      ],
      "metadata": {
        "id": "_FZ15m4xXnKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending on the situation, you may choose to remove outliers to prevent them from skewing your results or transform them to minimize their impact.\n",
        "\n",
        "**Step 4: Identify data entry errors*\n",
        "\n",
        "Data entry errors, such as incorrect values or inconsistent formatting, can be subtle but impactful.\n",
        "\n",
        "**Check for inconsistent or erroneous data**"
      ],
      "metadata": {
        "id": "hPRiESE8XpME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for unique values in categorical columns to identify inconsistencies\n",
        "print(df['CategoryColumn'].unique())  # Replace with actual column name\n",
        "\n",
        "# Use value counts to identify unusual or erroneous entries\n",
        "print(df['CategoryColumn'].value_counts())\n",
        "\n",
        "# Check numeric columns for impossible values (e.g., negative ages)\n",
        "print(df[df['Age'] < 0])  # Replace “Age” with the actual column name"
      ],
      "metadata": {
        "id": "d9nuxE7_XtX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By examining the unique values and frequency distributions in categorical columns, you can identify inconsistencies, such as misspellings or unexpected categories. For numeric data, you can look for impossible or implausible values that may indicate data entry errors.\n",
        "\n",
        "**Correct data entry errors**\n",
        "\n",
        "After identifying these errors, you can correct them as follows:\n",
        "\n",
        "**A. Standardize categories:**"
      ],
      "metadata": {
        "id": "bNEsXpAhXvX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['CategoryColumn'] = df['CategoryColumn'].str.strip().str.lower().replace({'misspelled': 'correct'})  # Example replacement"
      ],
      "metadata": {
        "id": "ZnEdZZf0Xy4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**B. Correct numeric errors:**\n",
        "\n"
      ],
      "metadata": {
        "id": "fFv2bTuAX0S8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Age'] = np.where(df['Age'] < 0, np.nan, df['Age'])  # Replace negative ages with NaN"
      ],
      "metadata": {
        "id": "3exMvibRX17B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardizing categories ensures consistency in your data, while correcting numeric errors prevents them from negatively impacting your analysis.\n",
        "\n",
        "**Step 5: Validate data consistency**\n",
        "\n",
        "Consistency checks help to ensure that your data behaves as expected over time or across different variables.\n",
        "\n",
        "**Perform consistency checks**"
      ],
      "metadata": {
        "id": "RqDHru2mX306"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validate data consistency between related columns\n",
        "df['Total'] = df['Part1'] + df['Part2']  # Replace with actual column names\n",
        "inconsistent_rows = df[df['Total'] != df['ExpectedTotal']]  # Replace with the actual column for the expected total\n",
        "print(inconsistent_rows)\n",
        "\n",
        "# Check for duplicate rows\n",
        "duplicates = df[df.duplicated()]\n",
        "print(duplicates)"
      ],
      "metadata": {
        "id": "aaqtWDhWX8_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-validation between related columns can identify inconsistencies, such as mismatched totals. Additionally, checking for duplicate rows helps to identify errors where the same data has been entered multiple times.\n",
        "\n",
        "**Address inconsistencies**\n",
        "\n",
        "Correct inconsistencies in the following ways:\n",
        "\n",
        "**A. Correct calculation errors:**"
      ],
      "metadata": {
        "id": "YnY0igJEX-1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['ExpectedTotal'] = df['Part1'] + df['Part2']  # Recalculate totals if they were incorrectly entered"
      ],
      "metadata": {
        "id": "iAOpbjOpYCvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**B. Remove duplicates:**\n",
        "\n"
      ],
      "metadata": {
        "id": "KbpmXPHqYEFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_no_duplicates = df.drop_duplicates()"
      ],
      "metadata": {
        "id": "OBw-87hXYF9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By recalculating values and removing duplicates, you ensure that your dataset is consistent and free from redundancy.\n",
        "\n",
        "# **Conclusion**\n",
        "\n",
        "- Identifying and addressing errors in data collection is a critical step in preparing high-quality datasets for analysis and ML. This walkthrough provided a comprehensive guide to detecting and correcting common data collection errors, including missing values, outliers, data entry errors, and inconsistencies.\n",
        "\n",
        "- By applying these techniques, you can significantly improve the reliability and accuracy of your data, leading to better outcomes in your AI and ML projects. As you continue to work with data, these skills will help you maintain the integrity of your datasets, ensuring that your models are built on a solid foundation of clean, accurate data."
      ],
      "metadata": {
        "id": "QrB1ro2ZYIV1"
      }
    }
  ]
}