{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXJGjRxGNxCyQW/sP+2Kon",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luis-arrieta/Foundations-of-AI-and-Machine-Learning/blob/main/Data_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Data Preprocessing**"
      ],
      "metadata": {
        "id": "_TdLJV6iHhSc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By the end of this activity, you will be able to:\n",
        "\n",
        "- Load and create a dummy dataset for preprocessing.\n",
        "\n",
        "- Apply data cleaning techniques to handle missing values and outliers.\n",
        "\n",
        "- Scale numeric features and encode categorical variables for machine learning.\n",
        "\n",
        "- Save the cleaned and preprocessed data for future use."
      ],
      "metadata": {
        "id": "ywPKqDNiHnl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Loading the dummy data\n",
        "\n",
        "- First, let's create a set of dummy data that simulates a typical dataset you might encounter in an ML project. For this example, you will generate a DataFrame with numeric and categorical data, some missing values, and a few outliers."
      ],
      "metadata": {
        "id": "IC9Doi4iH0Gl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step-by-step guide:**\n",
        "- **Step 1:** Generate and load the dummy data"
      ],
      "metadata": {
        "id": "fov-jZvtH5-b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xc7uoUrXHfWK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a dummy dataset\n",
        "np.random.seed(0)\n",
        "dummy_data = {\n",
        "    'Feature1': np.random.normal(100, 10, 100).tolist() + [np.nan, 200],  # Normally distributed with an outlier\n",
        "    'Feature2': np.random.randint(0, 100, 102).tolist(),  # Random integers\n",
        "    'Category': ['A', 'B', 'C', 'D'] * 25 + [np.nan, 'A'],  # Categorical with some missing values\n",
        "    'Target': np.random.choice([0, 1], 102).tolist()  # Binary target variable\n",
        "}\n",
        "\n",
        "# Convert the dictionary to a pandas DataFrame\n",
        "df_dummy = pd.DataFrame(dummy_data)\n",
        "\n",
        "# Display the first few rows of the dummy dataset\n",
        "print(df_dummy.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation: This code generates a dummy dataset with 100 rows and 4 columns: two numeric features, one categorical feature, and a binary target variable. The dataset includes some missing values and an outlier to simulate real-world data challenges."
      ],
      "metadata": {
        "id": "ctlw-ZeZIBA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Applying the preprocessing tool\n",
        "- Next, use the preprocessing tool you set up in the previous lesson to clean and preprocess this dummy data, making it ready for ML.\n",
        "\n",
        "**Step 2:** Load the preprocessing tool\n",
        "- Ensure your preprocessing functions are loaded into your environment. These functions include handling missing values, removing outliers, scaling data, and encoding categorical variables."
      ],
      "metadata": {
        "id": "vWjVx56xIGI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(df):\n",
        "    return df\n",
        "\n",
        "def handle_missing_values(df):\n",
        "    return df.fillna(df.mean())  # For numeric data, fill missing values with the mean\n",
        "\n",
        "def remove_outliers(df):\n",
        "    z_scores = np.abs(stats.zscore(df.select_dtypes(include=[np.number])))\n",
        "    return df[(z_scores < 3).all(axis=1)]  # Remove rows with any outliers\n",
        "\n",
        "def scale_data(df):\n",
        "    scaler = StandardScaler()\n",
        "    df[df.select_dtypes(include=[np.number]).columns] = scaler.fit_transform(df.select_dtypes(include=[np.number]))\n",
        "    return df\n",
        "\n",
        "def encode_categorical(df, categorical_columns):\n",
        "    return pd.get_dummies(df, columns=categorical_columns)\n",
        "\n",
        "def save_data(df, output_filepath):\n",
        "    df.to_csv(output_filepath, index=False)"
      ],
      "metadata": {
        "id": "jjnb7OtSIM1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation: These functions encapsulate the core preprocessing tasks, making them reusable across different datasets. They will be applied to our dummy data."
      ],
      "metadata": {
        "id": "9_LjAgMhIPeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3:** Preprocess the dummy data\n",
        "\n",
        "- Apply the preprocessing tool to the dummy data:"
      ],
      "metadata": {
        "id": "ZiqOGfgqISBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "df_preprocessed = load_data(df_dummy)\n",
        "\n",
        "# Handle missing values\n",
        "df_preprocessed = handle_missing_values(df_preprocessed)\n",
        "\n",
        "# Remove outliers\n",
        "df_preprocessed = remove_outliers(df_preprocessed)\n",
        "\n",
        "# Scale the data\n",
        "df_preprocessed = scale_data(df_preprocessed)\n",
        "\n",
        "# Encode categorical variables\n",
        "df_preprocessed = encode_categorical(df_preprocessed, ['Category'])\n",
        "\n",
        "# Display the preprocessed data\n",
        "print(df_preprocessed.head())"
      ],
      "metadata": {
        "id": "15uC0cdjIWG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation: This code applies the preprocessing steps to the dummy data. It handles missing values by filling them with the mean, removes outliers using the Z-score method, scales the numeric data, and encodes the categorical variables using one-hot encoding."
      ],
      "metadata": {
        "id": "ZSV1yNT0IZH_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Saving the preprocessed data\n",
        "- Finally, save the preprocessed data to a new comma-separated values (CSV) file for use in ML tasks.\n",
        "\n",
        "**Step 4:** Save the preprocessed data"
      ],
      "metadata": {
        "id": "ZT3tTY4TIa4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the cleaned and preprocessed DataFrame to a CSV file\n",
        "save_data(df_preprocessed, 'preprocessed_dummy_data.csv')\n",
        "\n",
        "print('Preprocessing complete. Preprocessed data saved as preprocessed_dummy_data.csv')"
      ],
      "metadata": {
        "id": "9V5Jcqp_Igs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation: Saving the preprocessed data to a new file ensures that it’s ready for use in training ML models. This step makes it easy to use the cleaned and processed data in future analysis or modeling efforts."
      ],
      "metadata": {
        "id": "WmIpa5m2Ii2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Verifying the preprocessing steps\n",
        "- After preprocessing, it’s important to verify that the data has been processed correctly:\n",
        "\n",
        "- Check for missing values:"
      ],
      "metadata": {
        "id": "wf8z3obFIkY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_preprocessed.isnull().sum())"
      ],
      "metadata": {
        "id": "G4HM4KTSImnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation: This checks that all missing values. have been handled properly."
      ],
      "metadata": {
        "id": "d0c8WUW8IoX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Verify outlier removal:"
      ],
      "metadata": {
        "id": "usokLpU5Ip9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_preprocessed.describe())"
      ],
      "metadata": {
        "id": "CuC6FggDIyGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation: This summarizes the dataset and confirms that any extreme values (outliers). have been removed.\n",
        "\n"
      ],
      "metadata": {
        "id": "qOqlz_VMIzpr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Inspect scaled data:"
      ],
      "metadata": {
        "id": "ieheoFsrI1MA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_preprocessed.head())"
      ],
      "metadata": {
        "id": "pD689dxEI2us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation: This ensures that the numeric features have been scaled properly, making them ready for ML algorithms"
      ],
      "metadata": {
        "id": "80K0PiflI4U0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Check categorical encoding:"
      ],
      "metadata": {
        "id": "djz08grSI5wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_preprocessed.columns)"
      ],
      "metadata": {
        "id": "folAVdxqI7D4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation: This confirms that the categorical variables have been encoded into numerical values correctly.\n",
        "\n"
      ],
      "metadata": {
        "id": "nvIe5mXdI8YC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "\n",
        "- By completing this activity, you have successfully used your data preprocessing tool to clean and prepare a set of dummy data for ML. This process included handling missing values, managing outliers, scaling numeric features, and encoding categorical variables. The preprocessed data is now ready for use in training ML models.\n",
        "\n",
        "- As you continue to work with real-world datasets, this preprocessing tool will be invaluable in ensuring that your data is clean, consistent, and properly formatted, ultimately leading to better-performing models. Continue to refine and adapt this tool to suit the specific needs of your projects, and you'll be ready to handle the challenges of data preprocessing in any ML workflow."
      ],
      "metadata": {
        "id": "qoZFJ_YnI-a0"
      }
    }
  ]
}